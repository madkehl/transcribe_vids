webinar on access this is a pre-recorded webinar so third webinar is data validation submission this webinar overlaps where the data organization webinar left off and is offered around the to submission Cycles which are January 15th and July 15th this webinar helps repair users to understand the causes of validation error handling submission and post admission quality assurance and quality control this webinar the deed access is the last in our series and discusses the process of requesting data and access and covers various methods to Corey package and download data from any of this webinar is offered around the two standard data sharing 15th and November 15th we will talk about competition credits we hope that you'll walk away with an understanding of how to request permission build queries in Access Data from NDA we also hope that you learned some tips along the way the Entity framework States how data is available to the research Community as soon as possible without compromising the ability of the authors to interpret and communicate formally their findings so this means we make available raw data so that researchers can see what they are coming from other projects as they plan primary or secondary analysis of their own and identify opportunities for collaboration on the request data access tab sometimes it takes so make sure to in the second column you'll see various permission groups and the corresponding descriptions each commission group government access to one and a repository which contains one or many collections of data from research projects if you already have approved access to the permission groups data under the status column you will see the number of days until your taxes expired if you don't have access the Box will be blank under actions you can click on the drop-down to request access this will bring up a window and walk you through the steps to gain access to data from that specific permission group all your active data request will be seen at the table at the top request for access to ship request for access to share data are typically processing up to 10 business days so it's approved will have access to the ND repository for 1 year after look for data other information in the NBA by entering Search terms and reviewing or filtering results based on the type of information to search page will display hits and provide some summer information specific to that type this is a good tool and seeing what is available based on key terms you might have in mind I want to switch over to the browser again to walk you through an example so here's the Search tool in for example we're just going to search for fmri and the right-hand box you can see the difference so content data archive website between data within the publication and underlying data Publications are the actual paper that were published using ND data and have an Associated DUI collection want to research project lab or Grant contributing data data structures represent a standardized definition of a measure or instrument that researchers used to collect and submit data data elements are the data value ranges and descriptions that make up the data structure experiments return different types of experiments related to your search and what collection they are part of Concepts return database an ontological Concepts these concepts are based on autism spectrum disorder phenotype ontology to find buy Alexa McRae at Harvard University so you'll see and (the number of results that are returned for each of these types and when you scroll down it'll show you the top 5 results and then you can load more to see the rest of them so what's the search for data to see what's available at a summary level it's time to actually start building queries and explored you have approved access for add an item level when ready to query data you should go back to the NDA home page and click on the get data tab there currently five methods used to build filters to query data there's data from Labs data from papers that a dictionary you can cry by Concepts or you can quit by Goods you can also add additional filters from experiments or select one of our featured data sets from Labs corresponds to collections and dated from papers corresponds to studies authenticated users find all data they have access to that is associated with a list of specific Goods anyone can visit the end a website and use the various query tools to Brown summary and general information on the data but those with approved access can view detailed subject level data as well as build data packages when you selected data that you want to package you will first added to your filter car which serves as sort of a shopping cart for your data if you make multiple Selections in a single filter for example and the data dictionary it will ply or logic and return all data that matches any selection let's say you have image of 3 and fmri results 01 with your data structures such a data from both data structures will be added to your filter carb are nuclear to also features of selected filter workplace which allows you to select data you want to package and review it before adding it to your filter car if you apply filters and subsequent transactions for example a collection and then a specific data structure it will use and logic and only returned data that matches all the selections therefore if add multiple filters to the car it will reduce the number of subjects available as they become more specific to your query this is why it's important to pay attention to the order you are building your queries while one way is not necessarily better than the other account affect the number of subjects or packaging the filter car allow you to edit or clear the selected filters as needed so not from our new query filter tool I'll show you how to make a query and add it to your filter car in this example I'm going to add data from two data structures in the same transaction so the filter cart will use or logic tat subjects from both data structures to the cart to the state of structure and added to the workspace I can also search for data structures so here all search for image 039 my workspace I can see both the structure that I've added and now I'm going to submit them to the filter cart in the same transaction depending on the number of subjects I've been putting in my filter card this could take a couple of moments now I'm going to add another filter and 1/2 filter by phenotypes here filter back Concepts so for this example I only want to add data to my cart on subjects that are depressed so I'm going to go ahead and select depressed I want to add this to my workspace at this point you can add more filters for the concepts if you want but for example we're just going to stick to this one and I'm going to go ahead and add it to my filter cart once it's been added we should see a reduction in the number of subjects in our filter cart again this might take a minute or so okay and it looks like it's done at return 0 subjects which just means that the subjects that filter that we added to the card don't return any subjects if this happens you can simply edit your filter car and remove the previous filter which will bring back the subjects into your filter car water filter card is updating again I'm going to switch back and show you how to actually create the package added all the data that you want to hear filter car it's time to create a package a package is tied to your user and contains data that you have access to and selected to download and access once you have used one or more query tools to add filters to your car for download you'll be able to view and edit filters in the car panel in the upper right hand corner of your page clicking packaged Act 2 study will take you to a landing page this is a page where you can view the data you currently have returned by a query the left panel displays all of the collections that contain subject data that satisfies the only collections in the left panel after you've added studies to your filter car all studies Lincoln directly to one or multiple Collections and individual level data is always housing collection not studies studies contain analyze data and experimental design metadata in your package which you can directly download or push into a database that you can then access virtually in the NDA Cloud this database is called a meander and allows you to work with data easily access raw almonds and imaging files and even submit analyze data back to end day without ever downloading the data and managing it locally updated files fees are associated raw and analyze data files such as only images for downloading later we will be going over the option to include Associated files and day does have a download limit of 5 terabytes for all users if you need to work with a data set that has Associated files larger than 5 terabytes you can compute in the cloud which will which will go over shortly to view all your packages you can go back to the year dashboard and select packages this will show you the package ID and the package name that you created you can also click on the drop-down to see which package has you have that are shared packages over how to create a package will want to access the data inside it there are several ways to do this and the first one we're going to go over is with a meander so amend our or miniature and our is an Oracle database hosted an Amazon web services for your data we sort of touched on this earlier when you create them in. You will receive an email with instructions and how to connect to your database to create them in. You're going to go back to your dashboard under packages the table under actions you're going to hit the drop down and click on create mindar it's going to ask you for a unique password that specific to your mandar does not have to be the same one you used to log into NDA and just like I mentioned it will send you an email confirmation with directions on how to connect to your database once you connected your database you be able to view the tables each structure in your package will translate to a table in the in the database which you will be able to connect to using the provided credentials and the S3 links table you will find the location of all the rich data files that just omex or Imaging files directly where they are stored in Amazon S3 they'll be listed in the table even if you package your data to exclude the associated files we will talk about how to act says these files in more detail in the following sections so now that we have talked more about accessing your package through a mandar will discuss methods to actually begin downloading and streaming the contents of your package the easiest way to download your data is by using download manager at. Jnlp file that can be downloaded from the tools tab know you will need to have Java install to use this tool you can also install are python package and the tools which includes a built-in command line download to lastly you can use the jupyter notebook On Target have paid which also makes use of NDA tools we will discuss all three methods and detail first ever go over download manager you can download this file from the ND homepage under the tools tab once you have downloaded the jnlp file you can open it up where we'll ask you to login using your NDA username and password it will then show you our data access terms which you should read over and once accepted you'll be able to view a list of your packages this tool will allow you to directly download your package to a location of your choice please note that for any package exceeding the size limit of 5T be willing to use an alternative access method this is why it is important to build specific filters when clearing the data if you have recently created your package and may still be displayed as having the status of creating package when you launch the manager you can update the download manager status by using the refresh cute button browse allows to select a new download destination the option at the bottom allows you to stop ongoing downloads start all selected downloads and delete or clear packages from the interface packages are tied to your account and will persist after you have downloaded them to use computational methods to access date in the cloud you'll need two things first you'll need a list of Esther locations where the data files are located and second you'll need temporary AWS tokens to obtain the Su locations you can look at the S3 links table in the middle are as explained previously you can also look at the day structure file Stooges image 03 to find those locations once you have the S3 locations you'll need to generate temporary AWS tokens know these tokens do expired for 24 hours but are free to regenerate tokens can be generated and download manager by clicking on tools and generate a list credentials you can also find Python and curl scripts in our GitHub page to generate the tokens as well now you can use any third-party tool to install your extra files using the temporary tokens or you can simply use NDA tools I need tools isn't easy to install python package that will allow you to download the data using command line argument and will take care of generating and refreshing your temporary tokens to install open up a terminal or command prompt you should verify you have a version of python and pip install on your machine then some install NBA tools to install the latest version of the package you can also clone a copy for my GitHub page and install from there once installed you can run download CMD help to see a list of options files are downloaded automatically in a folder called AWS_downloads in your home directory this can be configured for options for downloading your data files from S3 you can either enter the name of the file directly into the command line so that would be the S3 the full S3 pass you can pass in a. Txt file containing a list of S3 files to download so if you have a list of SD files and you can sleep in a text file and then passed out in as an argument you can pass in a data structure. Txt file so if in your package you had an image three data structure which lists a bunch of S3 links you can pass in that data structure file and the tool download the SD links from that file or you can pass in a package ID and from there it can download the files that you had packaged this allows you the flexibility of downloading a specific subset of your data package furthermore and a tools can be installed in any OS and python version so you can work with data directly in the cloud for example from your ec2 instance so now I'm going to switch over to the terminal and I want to show a small example of how the download CMD tool would work once you have any air tools properly installed you what you should already have this command line tool pre-installed with it so it's just download CMV and what the dash dash option you can see a list of the different arguments you can pass them so for my example I'm I have a sample links text file which just contains a couple of S3 links that I want to download and so I'm going to do download CMD the name of the file I'm going to specify that it's a text file. Txt file with the dasht-e argument and I'm also going to pass in Dashi be just so it's print it prints out the file as it's downloading but that's optional and I'm going to go ahead and hit enter and then if I look in my finder I can see the files are downloading and I just had it set to the default aw_download but again you could figure that if you passed in this â€“ the option to change the directory so that's it's pretty simple to use and you can also pass in like I said you can pass in the S3 link directly so for example if he just passed this and directly or you can pass in a day to structure or package ID the last method I want to go over is the jupyter notebook notebook is available on our page this notebook will allow you to work with NDA Tools in a more interactive format provides the same for download options as the download CMD command line tool does this could also serves as a starting point for users to write their own Python scripts using ended tools that will allow them to download data from any age rules as from NDA as part of a larger pipeline so once you have your notebook setup and the file downloaded you can start the jupyter notebook and we have some directions and height invalid and submit data to see if you just grow towards the bottom under the download section you can use this to download data directly so you just have to run every kernel here you can see that we're importing certain classes from NDA tools and ear I'm going to use a data structures by which is image of 3 and when I go ahead and click submit and then here I'll just make the smaller I'm going to run this and it prints out the S3 file that's going to download it here you can generate the tokens and I'll print out a creepy sticky session token and then you can click on start downloads and it'll start downloading the files so it's downloading I'm going to open up my terminal and it also default to the AWS downloads directory at least you can see that's where I had it configured in the script but again this is something that you can change if you would like in your code and we can see the files are downloaded here and also listed here as a download so the last top of her going to cover today is computational credits the MTA has a computational credits pilot program which aims to provide an efficient model for scientist to request and receive approval for computational access okay thank you everybody for about 20 minutes left and we'll be happy to answer any questions so we'll wait around are we had a question about showing us again how to add a second layer of the filters we can go back and Replay that at 4 now we will begin the NDA data access webinar focused primarily for researchers who are looking to build new research data sets using human subjects data from hundreds of projects house I'm so to answer there was a question from us to have us go over adding a second layer of filters so we were just trying to bring up that portion of the webinar to show again I can talk through adding the second layer filters I think I have to make myself presenter bear with me okay so before submitting to the filter car or I can submit to the public right now and I can come back I can show an example of classic builders I had an example where we had more than one picture at a time so here I selected two items from this category of data dictionary to add to the filter car if we go there and look at our raisin on my browser I haven't seen any additional questions in the last 3 minutes I still see we have people on the line I think we'll wait another 5 minutes if anybody else else has any questions now. we haven't had any additional questions so we're going to go ahead and the webinar again we will follow up with 